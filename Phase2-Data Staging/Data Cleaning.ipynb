{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ottawa Traffic Accident Data Analytic- Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd # geojson library\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "from geopy.geocoders import Nominatim#use geopy library to get \n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "import json\n",
    "from shapely.ops import nearest_points\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import traffic dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### file path of ottawa traffic accident data 2014-2017 \n",
    "collision_2013 = 'C:/Users/project file/collisionsOttawa2013.csv'\n",
    "collision_2014 = 'C:/Users/project file/collisionsOttawa2014.csv'\n",
    "collision_2015 = 'C:/Users/project file/collisionsOttawa2015.csv'\n",
    "collision_2016 = 'C:/Users/project file/collisionsOttawa2016.csv'\n",
    "collision_2017 = 'C:/Users/project file/collisionsOttawa2017.csv'\n",
    "# read csv to dataframe\n",
    "df_2013 = pd.read_csv(collision_2013)\n",
    "df_2014 = pd.read_csv(collision_2014)\n",
    "df_2015 = pd.read_csv(collision_2015)\n",
    "df_2016 = pd.read_csv(collision_2016)\n",
    "df_2017 = pd.read_csv(collision_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58345 entries, 0 to 58344\n",
      "Data columns (total 14 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   COLLISION_ID              58345 non-null  object \n",
      " 1   LOCATION                  58345 non-null  object \n",
      " 2   X                         58345 non-null  float64\n",
      " 3   Y                         58345 non-null  float64\n",
      " 4   LONGITUDE                 58345 non-null  float64\n",
      " 5   LATITUDE                  58345 non-null  float64\n",
      " 6   DATE                      58345 non-null  object \n",
      " 7   TIME                      58345 non-null  object \n",
      " 8   ENVIRONMENT               58344 non-null  object \n",
      " 9   LIGHT                     58345 non-null  object \n",
      " 10  SURFACE_CONDITION         58345 non-null  object \n",
      " 11  TRAFFIC_CONTROL           58303 non-null  object \n",
      " 12  COLLISION_CLASSIFICATION  58345 non-null  object \n",
      " 13  IMPACT_TYPE               58345 non-null  object \n",
      "dtypes: float64(4), object(10)\n",
      "memory usage: 6.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Make Consistency of data columns in each dataset\n",
    "df_2017 = df_2017.rename(columns={'Record':'COLLISION_ID'})\n",
    "df_2017 = df_2017.drop(columns=['Year','Collision_Location'])\n",
    "\n",
    "# Adjust the order of the data columns in the dataset 2017\n",
    "df_2017 = df_2017[['COLLISION_ID', 'LOCATION', 'X', 'Y', 'LONGITUDE', 'LATITUDE', 'DATE',\n",
    "       'TIME', 'ENVIRONMENT', 'LIGHT', 'SURFACE_CONDITION', 'TRAFFIC_CONTROL', 'COLLISION_CLASSIFICATION', 'IMPACT_TYPE']]\n",
    "\n",
    "# concatenating df_2015,df_2016,df_2014 axix = 0\n",
    "df_concatenated = pd.concat([df_2014,df_2015,df_2016], ignore_index=True)\n",
    "df_concatenated = df_concatenated.drop(columns=['TRAFFIC_CONTROL_CONDITION','NO_OF_PEDESTRIANS'])\n",
    "df_combine = pd.concat([df_concatenated,df_2017],ignore_index=True)\n",
    "\n",
    "df_combine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COLLISION_ID                 0\n",
       "LOCATION                     0\n",
       "X                            0\n",
       "Y                            0\n",
       "LONGITUDE                    0\n",
       "LATITUDE                     0\n",
       "DATE                         0\n",
       "TIME                         0\n",
       "ENVIRONMENT                  1\n",
       "LIGHT                        0\n",
       "SURFACE_CONDITION            0\n",
       "TRAFFIC_CONTROL             42\n",
       "COLLISION_CLASSIFICATION     0\n",
       "IMPACT_TYPE                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check missing values\n",
    "df_combine.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COLLISION_ID                0\n",
       "LOCATION                    0\n",
       "X                           0\n",
       "Y                           0\n",
       "LONGITUDE                   0\n",
       "LATITUDE                    0\n",
       "DATE                        0\n",
       "TIME                        0\n",
       "ENVIRONMENT                 0\n",
       "LIGHT                       0\n",
       "SURFACE_CONDITION           0\n",
       "TRAFFIC_CONTROL             0\n",
       "COLLISION_CLASSIFICATION    0\n",
       "IMPACT_TYPE                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill missing values\n",
    "df_combine.loc[df_combine.ENVIRONMENT.isnull(), 'ENVIRONMENT'] = '00 - Unknown'\n",
    "df_combine.loc[df_combine.TRAFFIC_CONTROL.isnull(),'TRAFFIC_CONTROL'] = '00 - Unknown'\n",
    "df_combine.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Nearest Neighbours to each accident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path of the ottawa neighbourhood_list.json\n",
    "file_ottawa_neighbourhood = 'C:/Users/project file/neighborhood_list.json'\n",
    "df_neighbour=pd.read_json(file_ottawa_neighbourhood)\n",
    "# convert the dataframe into suitable form\n",
    "list1 = [i for i in df_neighbour.columns]\n",
    "list2 = [i for i in df_neighbour.index]\n",
    "list3 = [i for i in df_neighbour[df_neighbour.index == 'lat'].values[0]]\n",
    "list4 = [i for i in df_neighbour[df_neighbour.index == 'lng'].values[0]]\n",
    "\n",
    "#create formal new formal dataframe for the neighbourhood\n",
    "df_neighbourhood = pd.DataFrame({'Neighbourhoods':list1,'LATITUDE':list3, 'LONGITUDE':list4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to combine the dataframe to calculate the distance between the accidents and neighbourhoods.\n",
    "def create_gdf(df, x='LONGITUDE', y='LATITUDE'):\n",
    "    return gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df[x],df[y]), crs={'init':'EPSG:4326'})\n",
    "\n",
    "accidents_gdf = create_gdf(df_combine)\n",
    "neighbourhood_gdf = create_gdf(df_neighbourhood)\n",
    "\n",
    "# define the methods to calculate the nearst neighbourhoods:\n",
    "def calculate_nearest(row, destination, val, col='geometry'):\n",
    "    #create unary union\n",
    "    dest_unary = destination['geometry'].unary_union\n",
    "    #find closest point\n",
    "    nearest_geom = nearest_points(row[col],dest_unary)\n",
    "    #find the corresponding geom\n",
    "    match_geom = destination.loc[destination.geometry==nearest_geom[1]]\n",
    "    #get the corresponding value\n",
    "    match_value = match_geom[val].to_numpy()[0]\n",
    "    return match_value\n",
    "\n",
    "#get the nearest geometry\n",
    "accidents_gdf['nearest_geom'] = accidents_gdf.apply(calculate_nearest, destination=neighbourhood_gdf,val='geometry',axis=1)\n",
    "#get the nearest neighbourhoodname:\n",
    "accidents_gdf['Neighbourhoods'] = accidents_gdf.apply(calculate_nearest, destination=neighbourhood_gdf,val='Neighbourhoods',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neighbourhoods\n",
       "Barrhaven                   3575\n",
       "Cumberland                   587\n",
       "Kanata                      5550\n",
       "Nepean                      8229\n",
       "Orleans                     5923\n",
       "Ottawa Central              2551\n",
       "Ottawa East                15084\n",
       "Ottawa South                5031\n",
       "Ottawa West Centre Town     2379\n",
       "Rural Ottawa South          2171\n",
       "Rural Ottawa West           4935\n",
       "Stittsville                 2330\n",
       "Name: COLLISION_ID, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have assign neighbourhood to the accidents, now we would like to look at the groupby count:\n",
    "accidents_gdf.groupby('Neighbourhoods').COLLISION_ID.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Nearest Weather Station\n",
    "we need to do the weather analysis with the accidents, so we need to match each weather condition per hour with the accidents. And We need to find the nearest weather station for the accidents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering\n",
    "station_file = 'C:/Users/project file/StationInventoryEN.csv'\n",
    "station_df = pd.read_csv(station_file,skiprows=3)\n",
    "station_df = station_df[['Name','Province','Station ID','Latitude (Decimal Degrees)','Longitude (Decimal Degrees)']]\n",
    "station_df= station_df[station_df['Province']=='ONTARIO'].reset_index(drop=True)\n",
    "station_df=station_df.rename(columns={'Latitude (Decimal Degrees)':'LATITUDE','Longitude (Decimal Degrees)':'LONGITUDE'})\n",
    "station_df = station_df[station_df['Name'].apply(lambda x: 'OTTAWA' in x)].reset_index(drop=True)\n",
    "station_list = [i for i in station_df.Name.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match the nearest weather based on the latitude and longitude with the traffic accident data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['OTTAWA U OF O', 'OTTAWA ROCKCLIFFE A', 'OTTAWA CITY HALL',\n",
       "       'OTTAWA HAZELDEAN', 'OTTAWA BRITANNIA', 'OTTAWA',\n",
       "       'OTTAWA ALBION RD', 'OTTAWA BILLINGS BRIDGE', 'OTTAWA NRC',\n",
       "       \"OTTAWA MACDONALD-CARTIER INT'L A\", 'OTTAWA BECKWITH RD',\n",
       "       'OTTAWA RIDEAU WARD', 'OTTAWA CDA', 'OTTAWA HOGS BACK',\n",
       "       'OTTAWA ALTA VISTA', 'OTTAWA KANATA', 'OTTAWA SOUTH MARCH',\n",
       "       'OTTAWA STOLPORT A', 'OTTAWA LEMIEUX ISLAND'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_gdf = create_gdf(station_df)\n",
    "#get the nearest Weather Station:\n",
    "accidents_gdf['Weather_station'] = accidents_gdf.apply(calculate_nearest, destination=station_gdf,val='Name',axis=1)\n",
    "accidents_gdf.Weather_station.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df = accidents_gdf.drop(columns = ['X','Y','geometry','nearest_geom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Weather Data Based on the nearest Weather Station "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist=['ontario_1_1','ontario_1_2','ontario_2_1-004','Ontario_2_2','Ontario_4-006']\n",
    "yearlist=[2013,2014,2015,2016,2017]\n",
    "root_dir = 'C:/Users/project file'\n",
    "weather_columns = ['X.Date.Time',\n",
    " 'Year',\n",
    " 'Month',\n",
    " 'Day',\n",
    " 'Time',\n",
    " 'Temp...C.',\n",
    " 'Temp.Flag',\n",
    " 'Dew.Point.Temp...C.',\n",
    " 'Dew.Point.Temp.Flag',\n",
    " 'Rel.Hum....',\n",
    " 'Rel.Hum.Flag',\n",
    " 'Wind.Dir..10s.deg.',\n",
    " 'Wind.Dir.Flag',\n",
    " 'Wind.Spd..km.h.',\n",
    " 'Wind.Spd.Flag',\n",
    " 'Visibility..km.',\n",
    " 'Visibility.Flag',\n",
    " 'Stn.Press..kPa.',\n",
    " 'Stn.Press.Flag',\n",
    " 'Hmdx',\n",
    " 'Hmdx.Flag',\n",
    " 'Wind.Chill',\n",
    " 'Wind.Chill.Flag',\n",
    " 'Weather.',\n",
    " 'X.U.FEFF..Station.Name.',\n",
    " 'X.Province.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getweather_ottawa():\n",
    "    df_weather=pd.DataFrame()\n",
    "    z = 0\n",
    "    entries = 0\n",
    "    for file in filelist:\n",
    "        print('reading '+str(file))\n",
    "        dtype = dict((i,convert_dtype) for i in weather_columns)\n",
    "        station_file = pd.read_csv('{}/Ontario/{}.csv'.format(root_dir,file), chunksize=1000000, low_memory=False)\n",
    "        #station_file['Year'] = station_file['Year'].astype('int')\n",
    "        #station_file['X.U.FEFF..Station.Name.'].astype('str')\n",
    "        for chunk in station_file:\n",
    "            print('processing...'+str(z))\n",
    "            y = chunk[(chunk['X.U.FEFF..Station.Name.'].isin(station_list))]\n",
    "            x = y[y['Year'].isin(yearlist)]\n",
    "            #set_index\n",
    "            x_size=x.size\n",
    "            #index=list(range(entries,x_size+entries))\n",
    "            x = x.reset_index(drop=True)\n",
    "            x['Weather_ID'] = x.index+entries\n",
    "            if x_size > 0:\n",
    "                #x.to_csv(root_dir+'/Chunk'+str(z)+'_weather.csv', encoding='utf-8', index=False)\n",
    "                print('Finish...'+str(z))\n",
    "                z+=1\n",
    "                entries += x_size\n",
    "                df_weather=df_weather.append(x,ignore_index=True)\n",
    "    return df_weather.to_csv(root_dir+'/ottawa_weather.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gongh\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (6,8,10,12,14,18,23) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_weather = pd.read_csv(root_dir+'/ottawa_weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1051776 entries, 0 to 1051775\n",
      "Data columns (total 27 columns):\n",
      " #   Column                   Non-Null Count    Dtype  \n",
      "---  ------                   --------------    -----  \n",
      " 0   X.Date.Time              1051776 non-null  object \n",
      " 1   Year                     1051776 non-null  int64  \n",
      " 2   Month                    1051776 non-null  int64  \n",
      " 3   Day                      1051776 non-null  int64  \n",
      " 4   Time                     1051776 non-null  object \n",
      " 5   Temp...C.                86735 non-null    float64\n",
      " 6   Temp.Flag                39 non-null       object \n",
      " 7   Dew.Point.Temp...C.      86737 non-null    float64\n",
      " 8   Dew.Point.Temp.Flag      8 non-null        object \n",
      " 9   Rel.Hum....              86721 non-null    float64\n",
      " 10  Rel.Hum.Flag             53 non-null       object \n",
      " 11  Wind.Dir..10s.deg.       85730 non-null    float64\n",
      " 12  Wind.Dir.Flag            167 non-null      object \n",
      " 13  Wind.Spd..km.h.          86739 non-null    float64\n",
      " 14  Wind.Spd.Flag            34 non-null       object \n",
      " 15  Visibility..km.          43788 non-null    float64\n",
      " 16  Visibility.Flag          0 non-null        float64\n",
      " 17  Stn.Press..kPa.          86730 non-null    float64\n",
      " 18  Stn.Press.Flag           37 non-null       object \n",
      " 19  Hmdx                     11147 non-null    float64\n",
      " 20  Hmdx.Flag                0 non-null        float64\n",
      " 21  Wind.Chill               25916 non-null    float64\n",
      " 22  Wind.Chill.Flag          0 non-null        float64\n",
      " 23  Weather.                 20263 non-null    object \n",
      " 24  X.U.FEFF..Station.Name.  1051776 non-null  object \n",
      " 25  X.Province.              1051776 non-null  object \n",
      " 26  Weather_ID               1051776 non-null  int64  \n",
      "dtypes: float64(12), int64(4), object(11)\n",
      "memory usage: 216.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_weather.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data format is not consitent based on the time format, we need to split the date to year,month,day based on the business requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# convert 12 timecounting to 24 timecounting\n",
    "def timeconvert(str1):\n",
    "    if str1[-2:] == \"AM\" and str1[:2] == \"12\":\n",
    "            return \"00\" + str1[2:-6]\n",
    "    elif str1[-2:] == \"AM\" and (str1[:2] =='11'or str1[:2]=='10'):\n",
    "            return str1[:-6]\n",
    "    elif str1[-2:] == \"AM\":\n",
    "            str1 = '0' + str1\n",
    "            return str1[:5]\n",
    "    elif str1[-2:] == \"PM\" and str1[:2] == \"12\":\n",
    "            return str1[:5]\n",
    "    elif str1[-2:] == \"PM\" and (str1[:2] =='11'or str1[:2]=='10'):\n",
    "            return str(int(str1[:2]) + 12) +str1[2:5]\n",
    "    else:\n",
    "            str1 = '0' + str1\n",
    "            return str(int(str1[:2]) + 12) + str1[2:5]\n",
    "    \n",
    "# convert datetime\n",
    "def str_to_datetime(x):\n",
    "    try:\n",
    "         return datetime.strptime(x,'%Y-%m-%d')\n",
    "    except:\n",
    "         return datetime.strptime(x,'%m/%d/%Y')\n",
    "\n",
    "# convert rounded hour\n",
    "def rounded_time(x):\n",
    "    global str\n",
    "    global int\n",
    "    if x[:2]=='23':\n",
    "        x = '23:00'\n",
    "        return x\n",
    "    else:\n",
    "        if int(x[-2:]) >=45 and x[0]=='0' and x[1]!='9':\n",
    "            return \"0\" + str(int(x[:2])+1)+':00'\n",
    "        elif int(x[-2:]) >=45:\n",
    "            return str(int(x[:2])+1)+':00'\n",
    "        else:\n",
    "            return x[:3]+'00'\n",
    "            \n",
    "            \n",
    "## get date and time columns\n",
    "def get_timecolumns(df):\n",
    "    global str\n",
    "    global int\n",
    "    # str_to date\n",
    "    df['DATE'] = df['DATE'].apply(lambda x: str_to_datetime(x))\n",
    "    # create time columns\n",
    "    df['YEAR'] = df['DATE'].apply(lambda x : x.year)\n",
    "    df['MONTH'] = df['DATE'].apply(lambda x : x.month)\n",
    "    df['DAY'] = df['DATE'].apply(lambda x: x.day)\n",
    "    df['TIME'] = df['TIME'].apply(lambda x : timeconvert(x))\n",
    "    df['ROUNDED_TIME'] = df['TIME'].apply(lambda x:rounded_time(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df['DATE']=accidents_df['DATE'].apply(lambda x:str(x)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = get_timecolumns(accidents_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_ID = [i for i in df_weather.index.values]\n",
    "df_weather['WEATHER_ID']=weather_ID\n",
    "df_weather = df_weather.drop(columns=['Weather_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 58345 entries, 0 to 58344\n",
      "Data columns (total 40 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   COLLISION_ID              58345 non-null  object        \n",
      " 1   LOCATION                  58345 non-null  object        \n",
      " 2   LONGITUDE                 58345 non-null  float64       \n",
      " 3   LATITUDE                  58345 non-null  float64       \n",
      " 4   DATE                      58345 non-null  datetime64[ns]\n",
      " 5   TIME                      58345 non-null  object        \n",
      " 6   ENVIRONMENT               58345 non-null  object        \n",
      " 7   LIGHT                     58345 non-null  object        \n",
      " 8   SURFACE_CONDITION         58345 non-null  object        \n",
      " 9   TRAFFIC_CONTROL           58345 non-null  object        \n",
      " 10  COLLISION_CLASSIFICATION  58345 non-null  object        \n",
      " 11  IMPACT_TYPE               58345 non-null  object        \n",
      " 12  Neighbourhoods            58345 non-null  object        \n",
      " 13  Weather_station           58345 non-null  object        \n",
      " 14  YEAR                      58345 non-null  int64         \n",
      " 15  MONTH                     58345 non-null  int64         \n",
      " 16  DAY                       58345 non-null  int64         \n",
      " 17  ROUNDED_TIME              58345 non-null  object        \n",
      " 18  X.Date.Time               58345 non-null  object        \n",
      " 19  Temp...C.                 0 non-null      float64       \n",
      " 20  Temp.Flag                 0 non-null      object        \n",
      " 21  Dew.Point.Temp...C.       0 non-null      float64       \n",
      " 22  Dew.Point.Temp.Flag       0 non-null      object        \n",
      " 23  Rel.Hum....               0 non-null      float64       \n",
      " 24  Rel.Hum.Flag              0 non-null      object        \n",
      " 25  Wind.Dir..10s.deg.        0 non-null      float64       \n",
      " 26  Wind.Dir.Flag             0 non-null      object        \n",
      " 27  Wind.Spd..km.h.           0 non-null      float64       \n",
      " 28  Wind.Spd.Flag             0 non-null      object        \n",
      " 29  Visibility..km.           0 non-null      float64       \n",
      " 30  Visibility.Flag           0 non-null      float64       \n",
      " 31  Stn.Press..kPa.           0 non-null      float64       \n",
      " 32  Stn.Press.Flag            0 non-null      object        \n",
      " 33  Hmdx                      0 non-null      float64       \n",
      " 34  Hmdx.Flag                 0 non-null      float64       \n",
      " 35  Wind.Chill                0 non-null      float64       \n",
      " 36  Wind.Chill.Flag           0 non-null      float64       \n",
      " 37  Weather.                  0 non-null      object        \n",
      " 38  X.Province.               58345 non-null  object        \n",
      " 39  WEATHER_ID                58345 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(14), int64(4), object(21)\n",
      "memory usage: 18.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_weather = df_weather.rename(columns={'Year':'YEAR','Month':'MONTH','Day':'DAY','Time':'ROUNDED_TIME','X.U.FEFF..Station.Name.':'Weather_station'})\n",
    "df_merged = pd.merge(new_df,df_weather, on=['YEAR','MONTH','DAY','ROUNDED_TIME','Weather_station'], how='left')\n",
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(root_dir+'/master_data.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
